{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Backpropagation & Gradient Descent (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Explain the intutition behind backproprogation\n",
    "* <a href=\"#p2\">Part 2</a>: Implement gradient descent + backpropagation on a feedforward neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Yesterday\n",
    "\n",
    "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinforce our understanding of those components and introduce the mechanics of training a neural network. Feed-forward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*_M4bZyuwaGby6KMiYVYXvg.jpeg\" width=\"400\"></center>\n",
    "\n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network: *Formal Summary*\n",
    "\n",
    "0. Pick a network architecture\n",
    "   - No. of input units = No. of features\n",
    "   - No. of output units = Number of Classes (or expected targets)\n",
    "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement code to compute a cost function $J(\\theta)$\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
    "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
    "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Calculating *\"cost\"*, *\"loss\"* or *\"error\"*\n",
    "\n",
    "We've talked about how in order to evaluate a network's performance, the data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it *should* have predicted. \n",
    "\n",
    "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value. \n",
    "\n",
    "We can summarize the overall quality of a network's predictions by finding the average error across all observations. This gives us the \"Mean Squared Error.\" which hopefully is a fairly familiar model evaluation metric by now. Graphing the MSE over each epoch (training cycle) is a common practice with Neural Networks. This is what you're seeing in the top right corner of the Tensorflow Playground website as the number of \"epochs\" climbs higher and higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an \"Epoch\"?\n",
    "\n",
    "An \"Epoch\" is one cycle of passing our data forward through the network, measuring error given our specified cost function, and then -via gradient descent- updating weights within our network to hopefully improve the quality of our predictions on the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch vs Minibatch vs Stochastic Gradient Descent Epochs\n",
    "\n",
    "You may have heard these variations on the training process referenced in the 3Blue1Brown videos about backpropagation. \"Minibatch\" Gradient Descent means that instead of passing all of our data through the network for a given epoch (Batch GD), we just pass a randomized portion of our data through the network for each epoch. \n",
    "\n",
    "Stochastic Gradient Descent is when we make updates to our weights after forward propagating each individual training observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about Hyperparameters\n",
    "\n",
    "Neural Networks have many more hyperparameters than other machine learning algorithms which is part of what makes them a beast to train.\n",
    "\n",
    "1. You need more data to train them on. \n",
    "2. They're complex so they take longer to train. \n",
    "3. They have lots and lots of hyperparameters which we need to find the most optimal combination of, so we might end up training our model dozens or hundreds of times with different combinations of hyperparameters in order to try and squeeze out a few more tenths of a percent of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Backpropagation (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walkthrough of the main intuitions and math behind the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Gradient?\n",
    "\n",
    "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
    "\n",
    "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In this section, we will again implement a multi-layer perceptron using numpy. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dm2HPETcrgy6",
    "toc-hr-collapsed": true
   },
   "source": [
    "![231 Neural Network](https://cdn-images-1.medium.com/max/1600/1*IjY3wFF24sK9UhiOlf36Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERyVgeO_IWyV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "# hours studying, hours sleep\n",
    "X = np.array(([2,9],\n",
    "              [1,5],\n",
    "              [3,6]), dtype=float)\n",
    "\n",
    "# Exam Scores\n",
    "y = np.array(([90],\n",
    "              [72],\n",
    "              [80]), dtype=float)\n",
    "\n",
    "# y = 2*hours_studying + 4*hours_sleeping + 50\n",
    "# ^ what the network is trying to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDeUBW6k4Ri4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying, Sleeping \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Test Score \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing Data on feature \n",
    "# Neural Network would probably do this on its own, but it will help us converge on a solution faster\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100\n",
    "\n",
    "print(\"Studying, Sleeping \\n\", X)\n",
    "print(\"Test Score \\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Neural Network Architecture\n",
    "Lets create a Neural_Network class to contain this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUI8VSR5zyBv"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3 \n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### Randomly Initialize Weights\n",
    "How many random weights do we need to initialize? \"Fully-connected Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IreIDe6P8H0H"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights: \n",
      " [[ 2.48783189  0.11697987 -1.97118428]\n",
      " [-0.48325593 -1.50361209  0.57515126]]\n",
      "Layer 2 weights: \n",
      " [[-0.20672583]\n",
      " [ 0.41271104]\n",
      " [-0.57757999]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 weights: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbxDhyjQ-RwS"
   },
   "source": [
    "### Implement Feedforward Functionality\n",
    "\n",
    "After this step our neural network should be able to generate an output even though it has not been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gGivpEk-VdP"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3 \n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weight Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activation\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weighted Sum 2\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 1.        ],\n",
       "       [0.33333333, 0.55555556],\n",
       "       [1.        , 0.66666667]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1pxdfmDAaJg"
   },
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intput [0.66666667 1.        ]\n",
      "output [0.25814933]\n"
     ]
    }
   ],
   "source": [
    "# Try to make a prediction with our updated 'net\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "output = nn.feed_forward(X[0])\n",
    "print(\"intput\", X[0])\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V61yNmAB2T5"
   },
   "source": [
    "### Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64185067]\n"
     ]
    }
   ],
   "source": [
    "error = y[0] - output\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25814933]\n",
      " [0.33067192]\n",
      " [0.22642076]]\n",
      "[[0.64185067]\n",
      " [0.38932808]\n",
      " [0.57357924]]\n"
     ]
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "error_all = y - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26wgCLU0TLvy"
   },
   "source": [
    "Why is my error so big?\n",
    "\n",
    "My error is so big because my prediction is low.\n",
    "\n",
    "Why are my prediction low?\n",
    "\n",
    "Because either:\n",
    "\n",
    "  1) Second layer **weights** are low\n",
    "  \n",
    "  (or)\n",
    "  \n",
    "  2) Activations coming from the first layer are low\n",
    "  \n",
    "How are activations from the first layer determined? \n",
    "\n",
    "  1) By inputs - fixed\n",
    "  \n",
    "  2) by **weights** - variable\n",
    "  \n",
    "The only thing that I have control over throughout this process in order to increase the value of my final predictions is to either increase weights in layer 2 or increase weights in layer 1. \n",
    "\n",
    "Imagine that you could only change your weights by a fixed amount. Say you have .3 and you have to split that up and disperse it over your weights so as to increase your predictions as much as possible. (This isn't actually what happens, but it will help us identify which weights we would benefit the most from moving.)\n",
    "\n",
    "I need to increase weights of my model somewhere, I'll get the biggest bang for my buck if I increase weights in places where I'm already seeing high activation values -because they end up getting multiplied together before being passed to the sigmoid function. \n",
    "\n",
    "> \"Neurons that fire together, wire together\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_eyzItYIxgm"
   },
   "source": [
    "### Implement Backpropagation \n",
    "\n",
    "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
    "\n",
    "What in our model could be causing our predictions to suck so bad? \n",
    "\n",
    "Well, we know that our inputs (X) and outputs (y) are correct, if they weren't then we would have bigger problems than understanding backpropagation.\n",
    "\n",
    "We also know that our activation function (sigmoid) is working correctly. It can't be blamed because it just does whatever we tell it to and transforms the data in a known way.\n",
    "\n",
    "So what are the potential culprits for these terrible predictions? The **weights** of our model. Here's the problem though. I have weights that exist in both layers of my model. How do I know if the weights in the first layer are to blame, or the second layer, or both? \n",
    "\n",
    "Lets investigate. And see if we can just eyeball what should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights1\n",
      " [[-1.75351135  1.23279898  0.24464757]\n",
      " [-0.06568225  0.30190098  0.79723428]] \n",
      "---------\n",
      "hidden_sum\n",
      " [[-1.23468981  1.12376697  0.96033266]\n",
      " [-0.62099392  0.57865576  0.52445712]\n",
      " [-1.79729952  1.4340663   0.77613709]] \n",
      "---------\n",
      "activated_hidden\n",
      " [[0.22536165 0.75468678 0.7231884 ]\n",
      " [0.34955543 0.64075804 0.6281894 ]\n",
      " [0.14218011 0.8075341  0.68484697]] \n",
      "---------\n",
      "weights2\n",
      " [[ 1.23073545]\n",
      " [-1.52187331]\n",
      " [-0.25502715]] \n",
      "---------\n",
      "activated_output\n",
      " [[0.25814933]\n",
      " [0.33067192]\n",
      " [0.22642076]] \n",
      "---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'activated_output']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in attributes if i[:2]!= '__'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX"
   },
   "source": [
    "### Backpropagation (Simple Overview)\n",
    "\n",
    "Our model has 9 total weights (6 in the first layer, 3 in the last layer) that could be off.\n",
    "\n",
    "1) Calculate Error for a given each observation\n",
    "\n",
    "2) Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
    "\n",
    "3) Look at final layer weights to get an idea for which weights are helping pass desirable signals and which are stifling desirable signals\n",
    "\n",
    "4) Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Update Weights Based on Gradient\n",
    "\n",
    "Repeat steps 1-4 for every observation in a given batch, and then given the network's cost function, calculate its gradient using calculus and update weights associated with the (negative) gradient of the cost function. \n",
    "\n",
    "Remember that we have 9 weights in our network therefore the gradient that comes from our gradient descent calculation will be the vector that takes us in the most downward direction along some function in 9-dimensional hyperspace.\n",
    "\n",
    "\\begin{align}\n",
    "C(w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "\\end{align}\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork: \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes =3 \n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weight Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activation\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weighted Sum 2\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Back prop thru the network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply derivative of sigmoid to error\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)\n",
    "        \n",
    "        # z2 error: how much were our output layer weights off\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # z2 delta: how much were the weights off?\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.hidden_sum)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Let's look at the shape of the Gradient Componets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from reference import NeuralNetwork\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Error Associated with Each Observation \n",
    "aka how wrong were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52228666],\n",
       "       [0.30522145],\n",
       "       [0.39651516]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Gradient \n",
    "Simple interpretation - how much more sigmoid activation would have pushed us towards the right answer?\n",
    "\n",
    "`self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4992653 ],\n",
       "       [-0.34424557],\n",
       "       [-0.39096572]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.output_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37771334],\n",
       "       [0.41477855],\n",
       "       [0.40348484]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.sigmoid(nn.output_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23504597],\n",
       "       [0.2427373 ],\n",
       "       [0.24068482]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.sigmoidPrime(nn.output_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12276138],\n",
       "       [0.07408863],\n",
       "       [0.09543518]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Error\n",
    "Justice hasn't been served yet - tho. We still have neurons to blame. Let's go back another layer. \n",
    "\n",
    "`self.z2_error = self.o_delta.dot(self.weights2.T)`\n",
    "\n",
    "__Discussion:__ Why is this shape different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09965516, -0.13921965,  0.00976766],\n",
       "       [ 0.06014363, -0.08402149,  0.00589495],\n",
       "       [ 0.07747231, -0.10822991,  0.00759342]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Gradient\n",
    "For each observation, how much more sigmoid activation from this layer would have pushed us towards the right answer?\n",
    "\n",
    "`self.z2_delta = self.z2_error * self.sigmoidPrime(self.hidden_sum)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0237703 , -0.02476406,  0.002156  ],\n",
       "       [ 0.01479946, -0.01888784,  0.0014134 ],\n",
       "       [ 0.01928004, -0.02190505,  0.00186591]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.shape == nn.weights1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descent\n",
    "\n",
    "*Discussion:* Input to Hidden Weight Update\n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.33333333, 1.        ],\n",
       "       [1.        , 0.55555556, 0.66666667]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04006006, -0.04471037,  0.00377438],\n",
       "       [ 0.04484559, -0.04986067,  0.00418516]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(nn.z2_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion:* Hidden to Output Weight Update\n",
    "- Why is output the shape 3x1? \n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12513006],\n",
       "       [0.21169554],\n",
       "       [0.18087949]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.activated_hidden.T.dot(nn.o_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network (fo real this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.73733449]\n",
      " [0.75875845]\n",
      " [0.75936043]]\n",
      "Loss: \n",
      " 0.009871287050858673\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.74167911]\n",
      " [0.76258209]\n",
      " [0.76389173]]\n",
      "Loss: \n",
      " 0.009394181706353642\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.74569248]\n",
      " [0.76608579]\n",
      " [0.76805005]]\n",
      "Loss: \n",
      " 0.00898516941419665\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.74940332]\n",
      " [0.76930029]\n",
      " [0.77187052]]\n",
      "Loss: \n",
      " 0.008633715582136787\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.75283763]\n",
      " [0.77225297]\n",
      " [0.77538456]]\n",
      "Loss: \n",
      " 0.008331018267449948\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.82369328]\n",
      " [0.78827597]\n",
      " [0.80783077]]\n",
      "Loss: \n",
      " 0.00351521465991658\n",
      "+---------EPOCH 2000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.83505406]\n",
      " [0.77933452]\n",
      " [0.80115774]]\n",
      "Loss: \n",
      " 0.0025799666271906396\n",
      "+---------EPOCH 3000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.84919165]\n",
      " [0.75902467]\n",
      " [0.80253225]]\n",
      "Loss: \n",
      " 0.0013702753012260057\n",
      "+---------EPOCH 4000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.86025222]\n",
      " [0.74640596]\n",
      " [0.80297126]]\n",
      "Loss: \n",
      " 0.0007619963018990826\n",
      "+---------EPOCH 5000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.86886747]\n",
      " [0.73924157]\n",
      " [0.80212766]]\n",
      "Loss: \n",
      " 0.00044799982918853845\n",
      "+---------EPOCH 6000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.8753705 ]\n",
      " [0.73445545]\n",
      " [0.80152643]]\n",
      "Loss: \n",
      " 0.0002726340798551219\n",
      "+---------EPOCH 7000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.8803291 ]\n",
      " [0.73107458]\n",
      " [0.80114495]]\n",
      "Loss: \n",
      " 0.00017030051848174242\n",
      "+---------EPOCH 8000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.88416788]\n",
      " [0.72862032]\n",
      " [0.80088246]]\n",
      "Loss: \n",
      " 0.00010858159717382756\n",
      "+---------EPOCH 9000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.88717709]\n",
      " [0.72679685]\n",
      " [0.80069204]]\n",
      "Loss: \n",
      " 7.036771770898717e-05\n",
      "+---------EPOCH 10000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n",
      "Predicted Output: \n",
      " [[0.88956062]\n",
      " [0.72541487]\n",
      " [0.80054946]]\n",
      "Loss: \n",
      " 4.620109735168149e-05\n"
     ]
    }
   ],
   "source": [
    "# Train my 'net\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In the module project, you will implement backpropagation inside a multi-layer perceptron (aka a feedforward neural network). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
    "\n",
    "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "A true Stochastic GD-based implementation from [Welch Labs](https://www.youtube.com/watch?v=bxe2T-V8XRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA9LaTgKr6rP"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_kHb6Se1u9y"
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYYVhFf4rn3q"
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "L-gYdVfgrysE",
    "outputId": "ae371bf9-692c-49b4-b165-8562dab9c06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 60\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 67\n"
     ]
    }
   ],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Jyv_L8Z2sKOA",
    "outputId": "08725651-6d21-401b-85c0-3487370b8bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.89995945]\n",
      " [0.72001094]\n",
      " [0.80000029]]\n",
      "Loss: \n",
      "5.879842854971768e-10\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "Gtf9WI9FtGPk",
    "outputId": "d062b2a3-5a92-403e-8ce0-c070aa79907b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "This is a reference implementation for you to explore. You will not be expected to apply it to today's module project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = 2*hours_studying + 4*hours_sleeping + 50\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(n=1000):\n",
    "    \n",
    "    study = np.random.uniform(1,5,(n,1))\n",
    "    sleep = np.random.uniform(1,6,(n,1))\n",
    "    \n",
    "    y = 2*study + 4*sleep + 50\n",
    "    \n",
    "    X = np.append(study, sleep, axis=1)\n",
    "    \n",
    "    X = X / np.amax(X, axis=0)\n",
    "    y = y / 100\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36107582, 0.4074156 ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(....))\n",
    "# model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 0s 399us/sample - loss: 0.0350 - mae: 0.1783 - mse: 0.0350\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 0.0311 - mae: 0.1669 - mse: 0.0311\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 0.0276 - mae: 0.1562 - mse: 0.0276\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0246 - mae: 0.1463 - mse: 0.0246\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0220 - mae: 0.1371 - mse: 0.0220\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s 35us/sample - loss: 0.0197 - mae: 0.1285 - mse: 0.0197\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0178 - mae: 0.1206 - mse: 0.0178\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0160 - mae: 0.1133 - mse: 0.0160\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 0.0145 - mae: 0.1065 - mse: 0.0145\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 0.0132 - mae: 0.1005 - mse: 0.0132\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 0.0121 - mae: 0.0952 - mse: 0.0121\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0111 - mae: 0.0903 - mse: 0.0111\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0102 - mae: 0.0860 - mse: 0.0102\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 0.0094 - mae: 0.0820 - mse: 0.0094\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 0.0087 - mae: 0.0785 - mse: 0.0087\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0081 - mae: 0.0752 - mse: 0.0081\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s 38us/sample - loss: 0.0076 - mae: 0.0723 - mse: 0.0076\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 0s 51us/sample - loss: 0.0071 - mae: 0.0698 - mse: 0.0071\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 0.0067 - mae: 0.0676 - mse: 0.0067\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s 54us/sample - loss: 0.0064 - mae: 0.0656 - mse: 0.0064\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0060 - mae: 0.0638 - mse: 0.0060\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s 37us/sample - loss: 0.0057 - mae: 0.0622 - mse: 0.0057\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s 53us/sample - loss: 0.0055 - mae: 0.0608 - mse: 0.0055\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 0.0053 - mae: 0.0596 - mse: 0.0053\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 0s 45us/sample - loss: 0.0051 - mae: 0.0585 - mse: 0.0051\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 0.0049 - mae: 0.0575 - mse: 0.0049\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s 38us/sample - loss: 0.0047 - mae: 0.0567 - mse: 0.0047\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 0s 45us/sample - loss: 0.0046 - mae: 0.0559 - mse: 0.0046\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s 55us/sample - loss: 0.0044 - mae: 0.0553 - mse: 0.0044\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 0.0043 - mae: 0.0547 - mse: 0.0043\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 0.0042 - mae: 0.0542 - mse: 0.0042\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s 35us/sample - loss: 0.0041 - mae: 0.0537 - mse: 0.0041\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 0.0041 - mae: 0.0533 - mse: 0.0041\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 0s 65us/sample - loss: 0.0040 - mae: 0.0529 - mse: 0.0040\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0039 - mae: 0.0525 - mse: 0.0039\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s 36us/sample - loss: 0.0039 - mae: 0.0522 - mse: 0.0039\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 0.0038 - mae: 0.0519 - mse: 0.0038\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0038 - mae: 0.0516 - mse: 0.0038\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0037 - mae: 0.0513 - mse: 0.0037\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s 38us/sample - loss: 0.0037 - mae: 0.0511 - mse: 0.0037\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s 57us/sample - loss: 0.0036 - mae: 0.0509 - mse: 0.0036\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 0.0036 - mae: 0.0507 - mse: 0.0036\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s 46us/sample - loss: 0.0036 - mae: 0.0506 - mse: 0.0036\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s 45us/sample - loss: 0.0035 - mae: 0.0504 - mse: 0.0035\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s 59us/sample - loss: 0.0035 - mae: 0.0503 - mse: 0.0035\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 0.0035 - mae: 0.0501 - mse: 0.0035\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 0s 32us/sample - loss: 0.0035 - mae: 0.0500 - mse: 0.0035\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s 72us/sample - loss: 0.0035 - mae: 0.0499 - mse: 0.0035\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 0.0034 - mae: 0.0498 - mse: 0.0034\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0034 - mae: 0.0497 - mse: 0.0034\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s 56us/sample - loss: 0.0034 - mae: 0.0496 - mse: 0.0034\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 0.0034 - mae: 0.0495 - mse: 0.0034\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0034 - mae: 0.0494 - mse: 0.0034\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 0.0034 - mae: 0.0494 - mse: 0.0034\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.0034 - mae: 0.0493 - mse: 0.0034\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 0s 54us/sample - loss: 0.0034 - mae: 0.0492 - mse: 0.0034\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 0s 31us/sample - loss: 0.0034 - mae: 0.0492 - mse: 0.0034\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 0.0034 - mae: 0.0491 - mse: 0.0034\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0034 - mae: 0.0491 - mse: 0.0034\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0033 - mae: 0.0490 - mse: 0.0033\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 0s 50us/sample - loss: 0.0033 - mae: 0.0490 - mse: 0.0033\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 0.0033 - mae: 0.0490 - mse: 0.0033\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 0.0033 - mae: 0.0489 - mse: 0.0033\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s 47us/sample - loss: 0.0033 - mae: 0.0489 - mse: 0.0033\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 0.0033 - mae: 0.0489 - mse: 0.0033\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s 38us/sample - loss: 0.0033 - mae: 0.0489 - mse: 0.0033\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s 41us/sample - loss: 0.0033 - mae: 0.0488 - mse: 0.0033\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 0.0033 - mae: 0.0488 - mse: 0.0033\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 0s 45us/sample - loss: 0.0033 - mae: 0.0488 - mse: 0.0033\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 0.0033 - mae: 0.0487 - mse: 0.0033\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 0s 32us/sample - loss: 0.0033 - mae: 0.0487 - mse: 0.0033\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 0s 47us/sample - loss: 0.0033 - mae: 0.0487 - mse: 0.0033\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s 36us/sample - loss: 0.0033 - mae: 0.0487 - mse: 0.0033\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s 36us/sample - loss: 0.0033 - mae: 0.0487 - mse: 0.0033\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 0.0033 - mae: 0.0487 - mse: 0.0033\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 0.0033 - mae: 0.0486 - mse: 0.0033\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 0.0033 - mae: 0.0486 - mse: 0.0033\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s 41us/sample - loss: 0.0033 - mae: 0.0486 - mse: 0.0033\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 0.0033 - mae: 0.0486 - mse: 0.0033\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 0.0033 - mae: 0.0486 - mse: 0.0033\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0033 - mae: 0.0486 - mse: 0.0033\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 0s 46us/sample - loss: 0.0033 - mae: 0.0486 - mse: 0.0033\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s 53us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s 54us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 0s 56us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 0s 62us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s 47us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 0s 51us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s 40us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 0.0033 - mae: 0.0485 - mse: 0.0033\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s 52us/sample - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 0s 46us/sample - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 0s 49us/sample - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 0s 48us/sample - loss: 0.0033 - mae: 0.0484 - mse: 0.0033\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(3, activation='sigmoid', input_dim=2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "results = model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mae', 'mse'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b0cd8f59b0>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Example with Tensorflow\n",
    "\n",
    "In this section, we will experiment with a very simple fashion classifier. \n",
    "\n",
    "It uses a data set called \"fashion mnist\", a set of small b&w images of apparel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs to be between 0 and 1\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(9):\n",
    "    # Define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    \n",
    "    # Plot raw pixel data\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique classes do we have?\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
       "        0.05098039, 0.28627451, 0.        , 0.        , 0.00392157,\n",
       "        0.01568627, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.00392157, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.        , 0.14117647,\n",
       "        0.53333333, 0.49803922, 0.24313725, 0.21176471, 0.        ,\n",
       "        0.        , 0.        , 0.00392157, 0.01176471, 0.01568627,\n",
       "        0.        , 0.        , 0.01176471],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
       "        0.8       , 0.69019608, 0.5254902 , 0.56470588, 0.48235294,\n",
       "        0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04705882, 0.03921569, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.60784314,\n",
       "        0.9254902 , 0.81176471, 0.69803922, 0.41960784, 0.61176471,\n",
       "        0.63137255, 0.42745098, 0.25098039, 0.09019608, 0.30196078,\n",
       "        0.50980392, 0.28235294, 0.05882353],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00392157, 0.        , 0.27058824, 0.81176471,\n",
       "        0.8745098 , 0.85490196, 0.84705882, 0.84705882, 0.63921569,\n",
       "        0.49803922, 0.4745098 , 0.47843137, 0.57254902, 0.55294118,\n",
       "        0.34509804, 0.6745098 , 0.25882353],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.00392157, 0.00392157, 0.        , 0.78431373, 0.90980392,\n",
       "        0.90980392, 0.91372549, 0.89803922, 0.8745098 , 0.8745098 ,\n",
       "        0.84313725, 0.83529412, 0.64313725, 0.49803922, 0.48235294,\n",
       "        0.76862745, 0.89803922, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.71764706, 0.88235294,\n",
       "        0.84705882, 0.8745098 , 0.89411765, 0.92156863, 0.89019608,\n",
       "        0.87843137, 0.87058824, 0.87843137, 0.86666667, 0.8745098 ,\n",
       "        0.96078431, 0.67843137, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.75686275, 0.89411765,\n",
       "        0.85490196, 0.83529412, 0.77647059, 0.70588235, 0.83137255,\n",
       "        0.82352941, 0.82745098, 0.83529412, 0.8745098 , 0.8627451 ,\n",
       "        0.95294118, 0.79215686, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.01176471, 0.        , 0.04705882, 0.85882353, 0.8627451 ,\n",
       "        0.83137255, 0.85490196, 0.75294118, 0.6627451 , 0.89019608,\n",
       "        0.81568627, 0.85490196, 0.87843137, 0.83137255, 0.88627451,\n",
       "        0.77254902, 0.81960784, 0.20392157],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02352941, 0.        , 0.38823529, 0.95686275, 0.87058824,\n",
       "        0.8627451 , 0.85490196, 0.79607843, 0.77647059, 0.86666667,\n",
       "        0.84313725, 0.83529412, 0.87058824, 0.8627451 , 0.96078431,\n",
       "        0.46666667, 0.65490196, 0.21960784],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01568627,\n",
       "        0.        , 0.        , 0.21568627, 0.9254902 , 0.89411765,\n",
       "        0.90196078, 0.89411765, 0.94117647, 0.90980392, 0.83529412,\n",
       "        0.85490196, 0.8745098 , 0.91764706, 0.85098039, 0.85098039,\n",
       "        0.81960784, 0.36078431, 0.        ],\n",
       "       [0.        , 0.        , 0.00392157, 0.01568627, 0.02352941,\n",
       "        0.02745098, 0.00784314, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.92941176, 0.88627451, 0.85098039,\n",
       "        0.8745098 , 0.87058824, 0.85882353, 0.87058824, 0.86666667,\n",
       "        0.84705882, 0.8745098 , 0.89803922, 0.84313725, 0.85490196,\n",
       "        1.        , 0.30196078, 0.        ],\n",
       "       [0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.24313725,\n",
       "        0.56862745, 0.8       , 0.89411765, 0.81176471, 0.83529412,\n",
       "        0.86666667, 0.85490196, 0.81568627, 0.82745098, 0.85490196,\n",
       "        0.87843137, 0.8745098 , 0.85882353, 0.84313725, 0.87843137,\n",
       "        0.95686275, 0.62352941, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.07058824,\n",
       "        0.17254902, 0.32156863, 0.41960784, 0.74117647, 0.89411765,\n",
       "        0.8627451 , 0.87058824, 0.85098039, 0.88627451, 0.78431373,\n",
       "        0.80392157, 0.82745098, 0.90196078, 0.87843137, 0.91764706,\n",
       "        0.69019608, 0.7372549 , 0.98039216, 0.97254902, 0.91372549,\n",
       "        0.93333333, 0.84313725, 0.        ],\n",
       "       [0.        , 0.22352941, 0.73333333, 0.81568627, 0.87843137,\n",
       "        0.86666667, 0.87843137, 0.81568627, 0.8       , 0.83921569,\n",
       "        0.81568627, 0.81960784, 0.78431373, 0.62352941, 0.96078431,\n",
       "        0.75686275, 0.80784314, 0.8745098 , 1.        , 1.        ,\n",
       "        0.86666667, 0.91764706, 0.86666667, 0.82745098, 0.8627451 ,\n",
       "        0.90980392, 0.96470588, 0.        ],\n",
       "       [0.01176471, 0.79215686, 0.89411765, 0.87843137, 0.86666667,\n",
       "        0.82745098, 0.82745098, 0.83921569, 0.80392157, 0.80392157,\n",
       "        0.80392157, 0.8627451 , 0.94117647, 0.31372549, 0.58823529,\n",
       "        1.        , 0.89803922, 0.86666667, 0.7372549 , 0.60392157,\n",
       "        0.74901961, 0.82352941, 0.8       , 0.81960784, 0.87058824,\n",
       "        0.89411765, 0.88235294, 0.        ],\n",
       "       [0.38431373, 0.91372549, 0.77647059, 0.82352941, 0.87058824,\n",
       "        0.89803922, 0.89803922, 0.91764706, 0.97647059, 0.8627451 ,\n",
       "        0.76078431, 0.84313725, 0.85098039, 0.94509804, 0.25490196,\n",
       "        0.28627451, 0.41568627, 0.45882353, 0.65882353, 0.85882353,\n",
       "        0.86666667, 0.84313725, 0.85098039, 0.8745098 , 0.8745098 ,\n",
       "        0.87843137, 0.89803922, 0.11372549],\n",
       "       [0.29411765, 0.8       , 0.83137255, 0.8       , 0.75686275,\n",
       "        0.80392157, 0.82745098, 0.88235294, 0.84705882, 0.7254902 ,\n",
       "        0.77254902, 0.80784314, 0.77647059, 0.83529412, 0.94117647,\n",
       "        0.76470588, 0.89019608, 0.96078431, 0.9372549 , 0.8745098 ,\n",
       "        0.85490196, 0.83137255, 0.81960784, 0.87058824, 0.8627451 ,\n",
       "        0.86666667, 0.90196078, 0.2627451 ],\n",
       "       [0.18823529, 0.79607843, 0.71764706, 0.76078431, 0.83529412,\n",
       "        0.77254902, 0.7254902 , 0.74509804, 0.76078431, 0.75294118,\n",
       "        0.79215686, 0.83921569, 0.85882353, 0.86666667, 0.8627451 ,\n",
       "        0.9254902 , 0.88235294, 0.84705882, 0.78039216, 0.80784314,\n",
       "        0.72941176, 0.70980392, 0.69411765, 0.6745098 , 0.70980392,\n",
       "        0.80392157, 0.80784314, 0.45098039],\n",
       "       [0.        , 0.47843137, 0.85882353, 0.75686275, 0.70196078,\n",
       "        0.67058824, 0.71764706, 0.76862745, 0.8       , 0.82352941,\n",
       "        0.83529412, 0.81176471, 0.82745098, 0.82352941, 0.78431373,\n",
       "        0.76862745, 0.76078431, 0.74901961, 0.76470588, 0.74901961,\n",
       "        0.77647059, 0.75294118, 0.69019608, 0.61176471, 0.65490196,\n",
       "        0.69411765, 0.82352941, 0.36078431],\n",
       "       [0.        , 0.        , 0.29019608, 0.74117647, 0.83137255,\n",
       "        0.74901961, 0.68627451, 0.6745098 , 0.68627451, 0.70980392,\n",
       "        0.7254902 , 0.7372549 , 0.74117647, 0.7372549 , 0.75686275,\n",
       "        0.77647059, 0.8       , 0.81960784, 0.82352941, 0.82352941,\n",
       "        0.82745098, 0.7372549 , 0.7372549 , 0.76078431, 0.75294118,\n",
       "        0.84705882, 0.66666667, 0.        ],\n",
       "       [0.00784314, 0.        , 0.        , 0.        , 0.25882353,\n",
       "        0.78431373, 0.87058824, 0.92941176, 0.9372549 , 0.94901961,\n",
       "        0.96470588, 0.95294118, 0.95686275, 0.86666667, 0.8627451 ,\n",
       "        0.75686275, 0.74901961, 0.70196078, 0.71372549, 0.71372549,\n",
       "        0.70980392, 0.69019608, 0.65098039, 0.65882353, 0.38823529,\n",
       "        0.22745098, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
       "        0.28235294, 0.16078431, 0.1372549 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# One hot encode outputs\n",
    "y_binary = to_categorical(y_train)\n",
    "y_binary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.6037 - accuracy: 0.7989 - val_loss: 0.5077 - val_accuracy: 0.8242\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.4626 - accuracy: 0.8411 - val_loss: 0.4725 - val_accuracy: 0.8338\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.4356 - accuracy: 0.8505 - val_loss: 0.4644 - val_accuracy: 0.8385\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4228 - accuracy: 0.8535 - val_loss: 0.4721 - val_accuracy: 0.8335\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.4131 - accuracy: 0.8568 - val_loss: 0.4520 - val_accuracy: 0.8426\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.4073 - accuracy: 0.8592 - val_loss: 0.4596 - val_accuracy: 0.8365\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.4018 - accuracy: 0.8604 - val_loss: 0.4533 - val_accuracy: 0.8431\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.3988 - accuracy: 0.8605 - val_loss: 0.4594 - val_accuracy: 0.8401\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.3947 - accuracy: 0.8621 - val_loss: 0.4451 - val_accuracy: 0.8443\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.3929 - accuracy: 0.8638 - val_loss: 0.4463 - val_accuracy: 0.8439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b0d4d0d1d0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       ...,\n",
       "       [1.9063035e-12, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NNF (Python3)",
   "language": "python",
   "name": "u4-s2-nnf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
